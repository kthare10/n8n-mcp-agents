version: "3.8"

networks:
  v4only:
    driver: bridge
    enable_ipv6: false

services:
  postgres:
    image: postgres:15
    container_name: n8n_postgres
    restart: always
    networks: [v4only]
    ports:
      - "127.0.0.1:5432:5432"   # expose locally
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-n8n}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-n8n}
      POSTGRES_DB: ${POSTGRES_DB:-n8n}
      TZ: ${TZ:-America/New_York}
    volumes:
      - ./db_data:/var/lib/postgresql/data

  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    restart: always
    depends_on: [postgres]
    networks: [v4only]
    ports:
      - "5678:5678"
    environment:
      N8N_SECURE_COOKIE: False
      TZ: ${TZ:-America/New_York}
      N8N_HOST: ${N8N_HOST:-localhost}
      N8N_PORT: ${N8N_PORT:-5678}
      N8N_PROTOCOL: ${N8N_PROTOCOL:-http}
      WEBHOOK_URL: ${WEBHOOK_URL:-https://YOUR_VM_IP:8443/}
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: True
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres          # use service name, not localhost
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: ${POSTGRES_DB:-n8n}
      DB_POSTGRESDB_USER: ${POSTGRES_USER:-n8n}
      DB_POSTGRESDB_PASSWORD: ${POSTGRES_PASSWORD:-n8n}
      N8N_ENCRYPTION_KEY: ${N8N_ENCRYPTION_KEY:-}
      N8N_PAYLOAD_SIZE_MAX: ${N8N_PAYLOAD_SIZE_MAX:-64}
      EXECUTIONS_MODE: ${EXECUTIONS_MODE:-regular}
      N8N_PUSH_BACKEND: websocket
      N8N_DEFAULT_CORS: False
      N8N_CORS_ALLOW_ORIGIN: ${N8N_CORS_ALLOW_ORIGIN:-https://YOUR_VM_IP:8443/}
    volumes:
      - n8n_data:/home/node/.n8n

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    networks: [v4only]
    ports:
      - "11434:11434"
    environment:
      GODEBUG: netdns=cgo            # make Go resolver follow /etc/gai.conf
      MODEL_NAME: ${MODEL_NAME:-llama3}
    volumes:
      - ./ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    entrypoint: ["/bin/sh","-c","ollama serve & sleep 2 && ollama pull $MODEL_NAME && tail -f /dev/null"]

  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    restart: always
    depends_on: [ollama]
    networks: [v4only]
    ports:
      - "8080:8080"
    environment:
      OLLAMA_BASE_URL: http://ollama:11434   # service name
    volumes:
      - ./webui_data:/app/backend/data

  nginx:
    image: nginx:1.27-alpine
    container_name: nginx
    restart: always
    depends_on: [open-webui, n8n, ollama]
    networks: [v4only]
    ports:
      - "80:80"
      - "443:443"
      - "8443:8443"     # if your config listens here
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
      - ./certs/fullchain.pem:/etc/ssl/public.pem:ro
      - ./certs/privkey.pem:/etc/ssl/private.pem:ro
      - /opt/data/production/logs/nginx/n8n:/var/log/nginx
    environment:
      GODEBUG: netdns=cgo

volumes:
  n8n_data: {}
